norm_cfg = dict(type='SyncBN', requires_grad=True)
img_size = 224

model = dict(
    type='Co2S',
    pretrained='pretrained/clip2mmseg_ViT16_clip_backbone.pth',
    backbone=dict(
        type='MaskClipVisionTransformer',
        img_size=(img_size, img_size),
        patch_size=16,
        patch_bias=False,
        in_channels=3,
        embed_dims=768,
        num_layers=12,
        num_heads=12,
        mlp_ratio=4,
        out_indices=[0, 4, 12],
        qkv_bias=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.0,
        with_cls_token=True,
        output_cls_token=False,
        norm_cfg=dict(type='LN', eps=1e-6),
        act_cfg=dict(type='GELU'),
        patch_norm=False,
        pre_norm=True,
        final_norm=True,
        return_clip_embed=True,
        return_qkv=True,
        interpolate_mode='bicubic',
        num_fcs=2,
        norm_eval=False
    ),
    
    decode_head=dict(
        type='VLGHead',
        img_size=img_size,
        num_classes=6,
        text_in_channels=512,
        text_channels=128,
        up_channels=(64, 32),
        skip_in_channels=(768, 768),
        skip_channels=(32, 16),
        skip_from_conv_feat=False,
        num_layers=2,
        num_heads=4,
        channels=128,
        pool_size=(4, 4),
        conv1_ksize=7,
        align_corners=False,
        loss_decode=None,
    ),
    freeze_backbone=True,
    exclude_keys=['attn'],
    # exclude_keys=None,
)